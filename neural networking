their are different hiddden layer(type is dense layer) in the neural net , named layer 1 , 2...... , layer 0 is the input layer and the last layer is the output layer
we have learned by forwared propogation with examples like digits/handwritting recogonation 
//tenserflow is one of the laeding framework to implement deep learning algorithum
x = np.array([200.0 , 290.0])
layer_1 = dense(unit = 3 , activation = 'sigmoid')
a1 = layer_1(x)
layer_2 = dense(unit = 1 , activation = 'sigmoid')
a2 = layer_2(a1)
model = seqential(layer_1 , layer_2) //after line 5 and 7
 seqential([dense(unit 3 , activation = 'sigmoid'),dense(unit 1 , activation = 'sigmoid')]) //avoid all the above line an d write this

//how to trsain the model -
1.specify the model which tell tensser flow how to compute for inference
2.compile the model using specific loss function
3.train the model

//common activation functions 
z = w bar ' x bar + b
1.linear activation function - g(z) = z
2. sigmoid - g(z) = 1/(1 + e*-z)
3.ReLU - g(z) = max(0 , z) if z < 0 then g(z) is 0 else g(z) is z

//how to choose the activation for different neural network
basically by keeping in mind what type of output your are trying to predict 
*for binary classification problem for chooises b/w 0 or 1 then use sigmoid activation function
*in case regression (like todays stock price will go up or down from tommorow either + or -) use linear activation function
*in case of regression (like price of house always +) use ReLU 
for hidden layer 
ReLU is by far the common choise

//why ReLU is most common in most of the places for hidden layer 
coz it's faster then sigmoid
and sigmoid function is flat at two part of the graph where as ReLU is one due to that gradient descent got slow in that parts

//why we need activation function in neural networks - coz then it won't solve anything else apart from linear regression model g(z) = z = w.x + b

//Multiple classification - here their is output more then binary 
Softmax - generalised 


//Advance optimization thrn gradient descent -
Adam (adaptive moment estimation) - algorithum - if it's see learning rate is too small and we are taking tiny strps to reach to the minima then it increases the value of learning rate to make the process faster (if it's going in the same direction)
if it's going back and forth then clearly learning rate too high then it decreases the value of learning rate so that we can reach the minima 

//CONVOLUTIONAL LAYER - WHEN NEURAL LOOK AT THE PART OF PREVIOUS LAYER INPUT -
WHY ? - FASTER THE COMPUTATION AND NEED LESS TRNING DATA SO LESS CHAMCE OF OVERFITTING
