their are different hiddden layer(type is dense layer) in the neural net , named layer 1 , 2...... , layer 0 is the input layer and the last layer is the output layer
we have learned by forwared propogation with examples like digits/handwritting recogonation 
//tenserflow is one of the laeding framework to implement deep learning algorithum
x = np.array([200.0 , 290.0])
layer_1 = dense(unit = 3 , activation = 'sigmoid')
a1 = layer_1(x)
layer_2 = dense(unit = 1 , activation = 'sigmoid')
a2 = layer_2(a1)
model = seqential(layer_1 , layer_2) //after line 5 and 7
 seqential([dense(unit 3 , activation = 'sigmoid'),dense(unit 1 , activation = 'sigmoid')]) //avoid all the above line an d write this

//how to trsain the model -
1.specify the model which tell tensser flow how to compute for inference
2.compile the model using specific loss function
3.train the model

//common activation functions 
z = w bar ' x bar + b
1.linear activation function - g(z) = z
2. sigmoid - g(z) = 1/(1 + e*-z)
3.ReLU - g(z) = max(0 , z) if z < 0 then g(z) is 0 else g(z) is z

//how to choose the activation for different neural network
basically by keeping in mind what type of output your are trying to predict 
*for binary classification problem for chooises b/w 0 or 1 then use sigmoid activation function
*in case regression (like todays stock price will go up or down from tommorow either + or -) use linear activation function
*in case of regression (like price of house always +) use ReLU 
for hidden layer 
ReLU is by far the common choise

//why ReLU is most common in most of the places for hidden layer 
coz it's faster then sigmoid
and sigmoid function is flat at two part of the graph where as ReLU is one due to that gradient descent got slow in that parts

//why we need activation function in neural networks - coz then it won't solve anything else apart from linear regression model g(z) = z = w.x + b

//Multiple classification - here their is output more then binary 
Softmax - generalised 


//Advance optimization thrn gradient descent -
Adam (adaptive moment estimation) - algorithum - if it's see learning rate is too small and we are taking tiny strps to reach to the minima then it increases the value of learning rate to make the process faster (if it's going in the same direction)
if it's going back and forth then clearly learning rate too high then it decreases the value of learning rate so that we can reach the minima 

//CONVOLUTIONAL LAYER - WHEN NEURAL LOOK AT THE PART OF PREVIOUS LAYER INPUT -
WHY ? - FASTER THE COMPUTATION AND NEED LESS TRNING DATA SO LESS CHAMCE OF OVERFITTING

//TRANING AND TESTING MODEL - IF THE GIVEN DAT FIT THE MODEL VERY WELL BUT FOR THE NEW EXAMPLES YOU ARE 
STRUGGLING TO GET LESS ERROR RESULT  THEN DIVIDE THE TRANING SETS INTO TWO PART ONE PART WITH TRAIN SET AND OTHER WITH THE TEST SET 
DIVIDE THE DATA  IN 3 PART TRAIN /CROSS VALIDATION /TEST SET
CROSS -VALIDATION SET - MEANS THIS IS THE EXTRA DATESET WHICH WE ARE  GOING TO USE TO CHECK THE VALIDITY AND ACCURACY OF DFFERENT MODELS

//VARIANCE/BYAS - 
UNDERFITTING ARE HIGH BIAS , HIGH Jcv , HIGH J train ,j cv = j train ,lemda of regularisation is low here , degree of polynomial is low , 
overfitting - high variance , low Jtrain , high Jcv ,j cv >  j train , lamda for regularisation is high ,degree of polynomial is high
get more traning set - fix high variance
get small set of features - fix high variance
try getting additional feature - fix hihg bias
try getting high polynomial features - fix high bias
increasing lemda of regularisation - fix high bias 

//IN NEURAL NETWORK HIGH BIAS/VARIANCE CAN BE REDUCE BY
TRANNING YOUR ALGORITHUM ON TRANING SET 
if dose not doing well then their is high bias problem solve by adding more hidden layer in the neural network
 dose it do well on cross validation set 
if not then their is high variance problem solved by retranning the neural network with more data

this is a great way to resolve high bias/variance problem only limitation is after a point it's hard to get more dat and 


//decision tree -
1.entropy - it is a measure of impurity at each node of the decision tree , nodes with large numbers of examples 
and still it's entropy is high is worst then once with less eample but high entropy
*in decision tree spliting the nodes are based on the features that reduce the 
entropy and lead to 100% purity at the end so , that no further division of nodes will tke palce
2. information gain - in decision tree reduction of enteopy is called information gain   
    choose the one with higher information gain is more pure
*but for most of the cases the single decision tree is highlly sensitive to data and to make this lss sensitive to data have multiple decision tree
----advatnages - it can adapt quickly to the data set
//random forest tree algorithum - use smaple with replacement lead to construct new traning set 

Random Forest vs Decision Tree: Key Differences.
//Decision Tree
 
As the name suggests, this algorithm builds its model in the structure of a tree along with decision nodes and leaf nodes. Here decision nodes are in order of two or more branches,
whereas the leaf node represents a decision. A decision tree is used to handle categorical and continuous data. It is a simple and effective decision-making diagram.

As one can see, trees are an easy and convenient way to visualize the results of algorithms and understand how decisions are made. The main advantage of a decision
tree is that it adapts quickly to the dataset. The final model can be viewed and interpreted in an orderly manner using a "tree" diagram. Conversely, since the random
forest algorithm builds many individual decision trees and then averages these predictions, it is much less likely to be affected by outliers.

 //random forest tree 
 
Also, a supervised machine learning algorithm works on both classification and regression tasks. The forest has almost the same 
hyperparameters as a decision tree. Its ensemble method of decision trees is generated on randomly split data. This entire group 
is a forest where each tree has a different independent random sample.

In the case of the random forest algorithm, many trees can make the algorithm too slow and inefficient for real-time prediction. 
In contrast, the results are generated based on randomly selected observations and features built on different decision trees in the random forest algorithm. 

Conversely, since random forests use only a few predictors to build each decision tree, the final decision trees tend to be decorrelated,
meaning that the random forest algorithm model is unlikely to outperform the dataset. As mentioned earlier, decision trees usually overwrite the training data - 
meaning they are more likely to match the "noise" in the dataset than the actual underlying model

//BOOSTING tree traning set- it is a boosting which focus on that part of the decision tree trained so far which were not working well and making them work on that subset of
traning set not doing well and boosting them through next decision tree

//XGBOOST-
*open source way form boosting tree
*improve the efficency and implementation
*good choise of default spliting and stoping spliting criteria
*built in regurarization for avoiding overfitting










